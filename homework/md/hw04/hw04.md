**Instructions**. Answer the following multiple choice questions by selecting all correct choices.

QUESTION 1. **Shuffling**

1.1 

What is *shuffling*?


( ) a method for recovering data after hardware failure
( ) the method used to ensure a random number generator is unbiased
( ) any movement of data
( ) moving data from memory to disk, usually caused by insufficient fast memory
(X) transferring data between nodes in a cluster, usually in order to complete a computation

1.2 
How can shuffling sometimes be reduced or avoided using Spark?

( ) use higher quality, fault-tolerant hardware
( ) use a pre-shuffled random number generator
( ) avoid algorithms that process the entire data set in favor of algorithms that only need a small subset of it
(X) partition an `RDD` before applying transformations or actions that cause shuffling
( ) use only fast memory, eliminating all spinning disks from the network

QUESTION 2. Partitions and Partitioning

2.1

Given a pair RDD (of key-value pairs), when we group values with the same key Spark collects key-value pairs with the same key on the same machine of our cluster.

(X) True
( ) False

2.2

By default, Spark uses range partitioning to determine which key-value pair should be sent to which machine.

( ) True
(X) False

2.3

Suppose we partition an RDD into a number of blocks.  From the following select the **two** statements that are true.

[ ]  A single block of the partition may be distributed across multiple machines in the cluster.
[X]  A block of the partition is assigned to at most one machine of the cluster.
[ ]  At least one block of the partition is assigned to every machine in the cluster.
[ ]  At most one block of the partition is assigned to every machine in the cluster.
[X]  More than one block of the partition may be assigned to the same machine in the cluster.


QUESTION 3.

Consider a Pair RDD, with keys [8, 23, 39, 40, 97], and suppose we want to partition these data into 4 blocks.
Using hash partitioning with the identity as `hashCode()` function `n.hashCode() == n`, check the boxes next to the numbers assigned to the given partition block.

3.1 block 0:

[X] 8
[ ]  23
[ ]  39
[X]  40
[ ]  97
[ ]  none

3.2 block 1:

[ ]  8
[ ]  23
[ ]  39
[ ]  40
[X]  97
[ ]  none


3.3 block 2:

[ ]  8
[ ]  23
[ ]  39
[ ]  40
[ ]  97
[X]  none

3.4 block 3:

[ ]  8
[X]  23
[X]  39
[ ]  40
[ ]  97
[ ]  none



QUESTION 4
Again, consider a Pair RDD with keys [8, 23, 39, 40, 97] and suppose we want to partition these data into 4 blocks.
Using range partitioning with ranges [0, 20], [21, 40], [41, 60], [61, 100], check the boxes next to the numbers assigned to the given partition block.

4.1 block 0:

[X]  8
[ ]  23
[ ]  39
[ ]  40
[ ]  97
[ ]  none

4.2 block 1:

[ ]  8
[X]  23
[X]  39
[X]  40
[ ]  97
[ ]  none


4.3 block 2:

[ ]  8
[ ]  23
[ ]  39
[ ]  40
[ ]  97
[X]  none


4.4 block 3:

[ ]  8
[ ]  23
[ ]  39
[ ]  40
[X]  97
[ ]  none


QUESTION 5. Which of the two strategies above results in a more balanced distribution of the data across the partition?

(X) hash partitioning
( ) range partitioning

[[With both strategies, three out of four blocks are occupied.  However, with range partitioning, block 1 has 3 elements, which is 3 times larger than the next biggest block, whereas, with hash partitioning, blocks 0 and 3 each have 2 elements, which makes them equal in size and 2 times larger than the next biggest block (block 1).  Each block of a partition is assigned to a single machine in the cluster. Suppose in addition we assume (a) no two blocks reside on the same machine and (b) the amount of work done by each machine is proportional to the amount of data assigned to that machine. Then it should be clear that, for the present example, hash partitioning will result in a more even distribution of work among machines in the cluster.]]

QUESTION 6.

6.1 Which method can we use to determine whether Spark recognizes that a transformation or action will result in shuffling?

( ) `debugDAG`
( ) `isShuffled`
( ) `showSchema`
( ) `showExecutionPlan`
(X) `toDebugString`


6.2

How data is initially partitioned and arranged on the cluster doesn't matter, since Spark will always re-arrange your data to avoid shuffling.

( ) True
(X) False

6.3 `reduceByKey` running on a pre-partitioned ROD will computed values locally, requiring only the final reduced values to be sent from workers to the driver.

(X) True
( ) False

6.4 `join` called on two RDDs that are pre-partitioned with the same partitioner and cached on the same node will cause the join to be computed locally, with no shuffling across the network.

(X) True
( ) False


6.5 Suppose algorithm **A** joins two RDDs and then performs a filter on the result while algorithm **B** performs a filter on the two RDDs and then joins the results.  Assume the two algorithms obtain the same result.  In general, which algorithm do you expect will cause less data shuffling?

( ) **A**
(X) **B**


QUESTION 7. Answer the following parts by selecting the words or phrase that best fits in the spaces provided.

7.1 In a *narrow dependency*, each block of the parent RDD may be used by `_________________` block(s) of the child RDD.

(X) "at most one" 
( ) "multiple"

7.2 Narrow dependencies are `_______________` since they require `_______________` of the data to be shuffled.

[X] "fast"
[ ] "slow" 
[ ] "some" 
[X] "none"


7.3 In a *wide dependency*, each block of the parent RDD may be used by `________________` block(s) of the child RDD.

( ) "at most one" 
(X) "multiple"

7.4 Wide dependencies are `_______________` since they require `_______________` of the data to be shuffled.

[ ] "fast" 
[X] "slow" 
[X] "some" 
[ ] "none"


QUESTION 8

8.1
The *query optimizer* of Spark SQL is called

(X) Catalyst
( ) Cobalt
( ) Map Reduce
( ) Platinum
( ) Tungsten

8.2 The *off-heap serializer* of Spark SQL is called

( ) Catalyst
( ) Cobalt
( ) Map Reduce
( ) Platinum
(X) Tungsten

